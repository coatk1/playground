{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Data](#Data)\n",
    "- [Text Wrap](#Text-Wrap)\n",
    "- [Counting](#Counting)\n",
    "- [NLTK](#NLTK)\n",
    "\n",
    "    - [NLTK Counting](#NLTK-Counting)\n",
    "    - [Filter](#Filter)\n",
    "    - [Line Tokenization](#Line-Tokenization)\n",
    "    - [Non-English Tokenization](#Non-English-Tokenization)\n",
    "    - [Word Tokenization](#Word-Tokenization)\n",
    "    - [Stopwords](#Stopwords)\n",
    "    - [Wordnet](#Wordnet)\n",
    "    - [Corpora](#Corpora)\n",
    "    - [Tagging Words](#Tagging-Words)\n",
    "    - [Text Classification](#Text-Classification)\n",
    "    - [Bigrams](#Bigrams)\n",
    "\n",
    "- [Strings](#Strings)\n",
    "- [Regex](#Regex)\n",
    "- [PrettyPrint](#Pretty-Print)\n",
    "- [Capitalization](#Capitalization)\n",
    "- [Spell Check](#Spell-Check)\n",
    "- [PDFs](#PDFs)\n",
    "- [Word Document](#Word-Document)\n",
    "- [RSS Feed](#RSS-Feed)\n",
    "\n",
    "Source: https://www.tutorialspoint.com/python_text_processing/index.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'In late summer 1945, guests are gathered for the wedding reception of Don Vito Corleones daughter Connie (Talia Shire) and Carlo Rizzi (Gianni Russo). Vito (Marlon Brando), the head of the Corleone Mafia family, is known to friends and associates as Godfather. He and Tom Hagen (Robert Duvall), the Corleone family lawyer, are hearing requests for favors because, according to Italian tradition, no Sicilian can refuse a request on his daughters wedding day.'\n",
    "\n",
    "FileName = (r\"data\\text.txt\")\n",
    "\n",
    "# fs = open(FileName, 'r')\n",
    "# data = fs.readlines()\n",
    "\n",
    "with open(FileName, 'r') as file:\n",
    "    data = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Summer is here.\\n',\n",
       " '  Sky is bright.\\n',\n",
       " '\\tBirds are gone.\\n',\n",
       " '\\t Nests are empty.\\n',\n",
       " '\\t  Where is Rain?\\n']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In late summer 1945, guests',\n",
       " 'are gathered for the wedding',\n",
       " 'reception of Don Vito',\n",
       " 'Corleones daughter Connie',\n",
       " '(Talia Shire) and Carlo Rizzi',\n",
       " '(Gianni Russo). Vito (Marlon',\n",
       " 'Brando), the head of the',\n",
       " 'Corleone Mafia family, is',\n",
       " 'known to friends and',\n",
       " 'associates as Godfather. He',\n",
       " 'and Tom Hagen (Robert Duvall),',\n",
       " 'the Corleone family lawyer,',\n",
       " 'are hearing requests for',\n",
       " 'favors because, according to',\n",
       " 'Italian tradition, no Sicilian',\n",
       " 'can refuse a request on his',\n",
       " 'daughters wedding day.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textwrap3 import wrap, dedent\n",
    "\n",
    "x = wrap(text1, 30)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Before Formatting**\n",
      " Summer is here.\n",
      "\n",
      "  Sky is bright.\n",
      "\n",
      "\tBirds are gone.\n",
      "\n",
      "\t Nests are empty.\n",
      "\n",
      "\t  Where is Rain?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"**Before Formatting**\")\n",
    "for i in range(len(data)):\n",
    "    print(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**After Formatting**\n",
      "Summer is here.\n",
      "Sky is bright.\n",
      "Birds are gone.\n",
      "Nests are empty.\n",
      "Where is Rain?\n"
     ]
    }
   ],
   "source": [
    "print(\"**After Formatting**\")\n",
    "for i in range(len(data)):\n",
    "    dedented_text = dedent(data[i]).strip()\n",
    "    print(dedented_text)\n",
    "# fs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Summer is here.\n",
      "  Sky is bright.\n",
      "\tBirds are gone.\n",
      "\t Nests are empty.\n",
      "\t  Where is Rain?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(FileName, 'r') as file:\n",
    "    lines_in_file = file.read()\n",
    "    print(lines_in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Summer', 'is', 'here.', 'Sky', 'is', 'bright.', 'Birds', 'are', 'gone.', 'Nests', 'are', 'empty.', 'Where', 'is', 'Rain?']\n",
      "Number of Words:  15\n"
     ]
    }
   ],
   "source": [
    "print(lines_in_file.split())\n",
    "print(\"Number of Words: \" , len(lines_in_file.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Corey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Summer', 'is', 'here', '.', 'Sky', 'is', 'bright', '.', 'Birds', 'are', 'gone', '.', 'Nests', 'are', 'empty', '.', 'Where', 'is', 'Rain', '?']\n",
      "Number of Words:  20\n"
     ]
    }
   ],
   "source": [
    "nltk_tokens = nltk.word_tokenize(lines_in_file)\n",
    "print(nltk_tokens)\n",
    "print(\"Number of Words: \" , len(nltk_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_data = \"The Sky is blue also the ocean is blue also Rainbow has a blue colour.\" \n",
    "\n",
    "# First Word tokenization\n",
    "nltk_tokens = nltk.word_tokenize(word_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Set\n",
    "no_order = list(set(nltk_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ocean',\n",
       " 'the',\n",
       " '.',\n",
       " 'also',\n",
       " 'is',\n",
       " 'colour',\n",
       " 'blue',\n",
       " 'Rainbow',\n",
       " 'has',\n",
       " 'a',\n",
       " 'The',\n",
       " 'Sky']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preserving Order\n",
    "ordered_tokens = set()\n",
    "result = []\n",
    "for word in nltk_tokens:\n",
    "    if word not in ordered_tokens:\n",
    "        ordered_tokens.add(word)\n",
    "        result.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Sky',\n",
       " 'is',\n",
       " 'blue',\n",
       " 'also',\n",
       " 'the',\n",
       " 'ocean',\n",
       " 'Rainbow',\n",
       " 'has',\n",
       " 'a',\n",
       " 'colour',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_data = \"The First sentence is about Python. The Second: about Django. You can learn Python,Django and Data Ananlysis here. \"\n",
    "nltk_tokens = nltk.sent_tokenize(sentence_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The First sentence is about Python.',\n",
       " 'The Second: about Django.',\n",
       " 'You can learn Python,Django and Data Ananlysis here.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-English Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_tokenizer = nltk.data.load('tokenizers/punkt/german.pickle')\n",
    "german_tokens = german_tokenizer.tokenize('Wie geht es Ihnen?  Gut, danke.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wie geht es Ihnen?', 'Gut, danke.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_data = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
    "nltk_tokens = nltk.word_tokenize(word_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " 'originated',\n",
       " 'from',\n",
       " 'the',\n",
       " 'idea',\n",
       " 'that',\n",
       " 'there',\n",
       " 'are',\n",
       " 'readers',\n",
       " 'who',\n",
       " 'prefer',\n",
       " 'learning',\n",
       " 'new',\n",
       " 'skills',\n",
       " 'from',\n",
       " 'the',\n",
       " 'comforts',\n",
       " 'of',\n",
       " 'their',\n",
       " 'drawing',\n",
       " 'rooms']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Corey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')\n",
    "stopwords.words()[620:680]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There\n",
      "tree\n",
      "near\n",
      "river\n"
     ]
    }
   ],
   "source": [
    "en_stops = set(stopwords.words('english'))\n",
    "\n",
    "all_words = ['There', 'is', 'a', 'tree','near','the','river']\n",
    "\n",
    "for word in all_words: \n",
    "    if word not in en_stops:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Corey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['locomotive', 'engine', 'locomotive_engine', 'railway_locomotive']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synset('locomotive.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a wheeled vehicle consisting of a self-propelled engine that is used to draw trains along railway tracks'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synset('locomotive.n.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['for your own good', \"what's the good of worrying?\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synset('good.n.01').examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('vertical.a.01.vertical'), Lemma('inclined.a.02.inclined')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.lemma('horizontal.a.01.horizontal').antonyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synonyms and Antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"Soil\"):\n",
    "    for lm in syn.lemmas():\n",
    "        if lm.antonyms():\n",
    "            antonyms.append(lm.antonyms()[0].name())\n",
    "        else:\n",
    "            synonyms.append(lm.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'begrime',\n",
       " 'bemire',\n",
       " 'colly',\n",
       " 'dirt',\n",
       " 'filth',\n",
       " 'grease',\n",
       " 'grime',\n",
       " 'ground',\n",
       " 'grunge',\n",
       " 'land',\n",
       " 'soil',\n",
       " 'stain',\n",
       " 'territory'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clean'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(antonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Corey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "fields = gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Poems by William Blake 1789]\n",
      "\n",
      " \n",
      "SONGS OF INNOCENCE AND OF EXPERIENCE\n",
      "and THE BOOK of THEL\n",
      "\n",
      "\n",
      " SONGS OF INNOCENCE\n",
      " \n",
      " \n",
      " INTRODUCTION\n",
      " \n",
      " Piping down the valleys wild,\n",
      "   Piping songs of pleasant glee,\n",
      " On a cloud I saw a child,\n",
      "   And he laughing said to me:\n",
      " \n",
      " \"Pipe a song about a Lamb!\"\n",
      "So I piped with merry cheer.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sample = gutenberg.raw(\"blake-poems.txt\")\n",
    "\n",
    "token = sent_tokenize(sample)\n",
    "\n",
    "for para in range(2):\n",
    "    print(token[para])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs\n",
      "<zip object at 0x00000146505B2648>\n"
     ]
    }
   ],
   "source": [
    "wlist = []\n",
    "\n",
    "for i in range(50):\n",
    "    wlist.append(token[i])\n",
    "\n",
    "wordfreq = [wlist.count(w) for w in wlist]\n",
    "print(\"Pairs\\n\" + str(zip(token, wordfreq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Corey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          may might  must  will \n",
      "hobbies   131    22    83   264 \n",
      "romance    11    51    45    43 \n",
      "  humor     8     8     9    13 \n"
     ]
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "          (genre, word)\n",
    "          for genre in brown.categories()\n",
    "          for word in brown.words(categories=genre))\n",
    "\n",
    "categories = ['hobbies', 'romance','humor']\n",
    "searchwords = [ 'may', 'might', 'must', 'will']\n",
    "\n",
    "cfd.tabulate(conditions=categories, samples=searchwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Corey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "text = nltk.word_tokenize(\"A Python is a serpent which eats eggs from the nest\")\n",
    "tagged_text = nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'DT'),\n",
       " ('Python', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('serpent', 'NN'),\n",
       " ('which', 'WDT'),\n",
       " ('eats', 'VBZ'),\n",
       " ('eggs', 'NNS'),\n",
       " ('from', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('nest', 'JJS')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\Corey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('tagsets')\n",
    "\n",
    "nltk.help.upenn_tagset('DT')\n",
    "nltk.help.upenn_tagset('NNP')\n",
    "nltk.help.upenn_tagset('VBZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[', 'JJ'), ('Poems', 'NNP'), ('by', 'IN'), ('William', 'NNP'), ('Blake', 'NNP'), ('1789', 'CD'), (']', 'NNP'), ('SONGS', 'NNP'), ('OF', 'NNP'), ('INNOCENCE', 'NNP'), ('AND', 'NNP'), ('OF', 'NNP'), ('EXPERIENCE', 'NNP'), ('and', 'CC'), ('THE', 'NNP'), ('BOOK', 'NNP'), ('of', 'IN'), ('THEL', 'NNP'), ('SONGS', 'NNP'), ('OF', 'NNP'), ('INNOCENCE', 'NNP'), ('INTRODUCTION', 'NNP'), ('Piping', 'VBG'), ('down', 'RP'), ('the', 'DT'), ('valleys', 'NN'), ('wild', 'JJ'), (',', ','), ('Piping', 'NNP'), ('songs', 'NNS'), ('of', 'IN'), ('pleasant', 'JJ'), ('glee', 'NN'), (',', ','), ('On', 'IN'), ('a', 'DT'), ('cloud', 'NN'), ('I', 'PRP'), ('saw', 'VBD'), ('a', 'DT'), ('child', 'NN'), (',', ','), ('And', 'CC'), ('he', 'PRP'), ('laughing', 'VBG'), ('said', 'VBD'), ('to', 'TO'), ('me', 'PRP'), (':', ':'), ('``', '``'), ('Pipe', 'VB'), ('a', 'DT'), ('song', 'NN'), ('about', 'IN'), ('a', 'DT'), ('Lamb', 'NN'), ('!', '.'), (\"''\", \"''\")]\n",
      "[('So', 'RB'), ('I', 'PRP'), ('piped', 'VBD'), ('with', 'IN'), ('merry', 'NNP'), ('cheer', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Tagging a corpus (see Corpora above)\n",
    "for i in token[:2]:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    tagged = nltk.pos_tag(words)\n",
    "    print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Corey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('movie_reviews')\n",
    "\n",
    "# Lets See how the movies are classified\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "all_cats = []\n",
    "for w in movie_reviews.categories():\n",
    "    all_cats.append(w.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meteor threat set to blow away all volcanoes & twisters !\n",
      "summer is here again !\n",
      "this season could probably be the most ambitious = season this decade with hollywood churning out films like deep impact , = godzilla , the x-files , armageddon , the truman show , all of which has but = one main aim , to rock the box office .\n",
      "leading the pack this summer is = deep impact , one of the first few film releases from the = spielberg-katzenberg-geffen's dreamworks production company .\n"
     ]
    }
   ],
   "source": [
    "fields = movie_reviews.fileids()\n",
    "\n",
    "sample = movie_reviews.raw(\"pos/cv944_13521.txt\")\n",
    "\n",
    "token = sent_tokenize(sample)\n",
    "for lines in range(4):\n",
    "    print(token[lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 77717), ('the', 76529), ('.', 65876), ('a', 38106), ('and', 35576), ('of', 34123), ('to', 31937), (\"'\", 30585), ('is', 25195), ('in', 21822)]\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "print(all_words.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'best'),\n",
       " ('best', 'performance'),\n",
       " ('performance', 'can'),\n",
       " ('can', 'bring'),\n",
       " ('bring', 'in'),\n",
       " ('in', 'sky'),\n",
       " ('sky', 'high'),\n",
       " ('high', 'success'),\n",
       " ('success', '.')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_data = \"The best performance can bring in sky high success.\"\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "\n",
    "list(nltk.bigrams(nltk_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line No-  0\n",
      " Summer is here.\n",
      "\n",
      "Line No-  1\n",
      "  Sky is bright.\n",
      "\n",
      "Line No-  2\n",
      "\tBirds are gone.\n",
      "\n",
      "Line No-  3\n",
      "\t Nests are empty.\n",
      "\n",
      "Line No-  4\n",
      "\t  Where is Rain?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data)):\n",
    "    print (\"Line No- \", i)\n",
    "    print (data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Summer is here.',\n",
       " '  Sky is bright.',\n",
       " '\\tBirds are gone.',\n",
       " '\\t Nests are empty.',\n",
       " '\\t  Where is Rain?']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.replace('\\n', '') for i in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse file order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.reverse()\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text2 = \"Please contact us at contact@tutorialspoint.com for further information.\"+\\\n",
    "        \" You can also give feedbacl at feedback@tp.com\"\n",
    "\n",
    "\n",
    "emails = re.findall(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+\", text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['contact@tutorialspoint.com', 'feedback@tp.com']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = 'Now a days you can learn almost anything by just visiting http://www.google.com. But if you are completely new to computers or internet then first you need to leanr those fundamentals. Next'+\\\n",
    "'you can visit a good e-learning site like - https://www.tutorialspoint.com to learn further on a variety of subjects.'\n",
    "\n",
    "urls = re.findall('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.google.com.', 'https://www.tutorialspoint.com']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(2, 5), match='tor'>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(\"tor\", \"Tutorial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not re.search(\"^tor\", \"Tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='Tut'>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match(\"Tut\", \"Tutorial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not re.match(\"tor\", \"Tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substitute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, You sohlud rcaeh the fiisnh lnie.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def replace(t):\n",
    "    inner_word = list(t.group(2))\n",
    "    random.shuffle(inner_word)\n",
    "    return t.group(1) + \"\".join(inner_word) + t.group(3)\n",
    "\n",
    "text = \"Hello, You should reach the finish line.\"\n",
    "\n",
    "print(re.sub(r\"(\\w)(\\w+)(\\w)\", replace, text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constrained Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main web Address:  https://www.tutorialspoint.com\n",
      "The protocol:  https\n",
      "The doman name:  www.tutorialspoint\n",
      "The TLD:  com\n"
     ]
    }
   ],
   "source": [
    "text = \"The web address is https://www.tutorialspoint.com\"\n",
    "\n",
    "# Taking \"://\" and \".\" to separate the groups \n",
    "result = re.search('([\\w.-]+)://([\\w.-]+)\\.([\\w.-]+)', text)\n",
    "\n",
    "if result :\n",
    "    print(\"The main web Address: \", result.group())\n",
    "    print(\"The protocol: \", result.group(1))\n",
    "    print(\"The doman name: \", result.group(2)) \n",
    "    print(\"The TLD: \", result.group(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretty Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "student_dict = {'Name': 'Tusar', 'Class': 'XII', \n",
    "     'Address': {'FLAT ':1308, 'BLOCK ':'A', 'LANE ':2, 'CITY ': 'HYD'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Name': 'Tusar',\n",
       " 'Class': 'XII',\n",
       " 'Address': {'FLAT ': 1308, 'BLOCK ': 'A', 'LANE ': 2, 'CITY ': 'HYD'}}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Address': {'BLOCK ': 'A',\n",
      "             'CITY ': 'HYD',\n",
      "             'FLAT ': 1308,\n",
      "             'LANE ': 2},\n",
      " 'Class': 'XII',\n",
      " 'Name': 'Tusar'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(student_dict,width=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp = {\"Name\":[\"Rick\",\"Dan\",\"Michelle\",\"Ryan\",\"Gary\",\"Nina\",\"Simon\",\"Guru\" ],\n",
    "   \"Salary\":[\"623.3\",\"515.2\",\"611\",\"729\",\"843.25\",\"578\",\"632.8\",\"722.5\" ],   \n",
    "   \"StartDate\":[ \"1/1/2012\",\"9/23/2013\",\"11/15/2014\",\"5/11/2014\",\"3/27/2015\",\"5/21/2013\",\n",
    "      \"7/30/2013\",\"6/17/2014\"],\n",
    "   \"Dept\":[ \"IT\",\"Operations\",\"IT\",\"HR\",\"Finance\",\"IT\",\"Operations\",\"Finance\"] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Name': ['Rick', 'Dan', 'Michelle', 'Ryan', 'Gary', 'Nina', 'Simon', 'Guru'],\n",
       " 'Salary': ['623.3', '515.2', '611', '729', '843.25', '578', '632.8', '722.5'],\n",
       " 'StartDate': ['1/1/2012',\n",
       "  '9/23/2013',\n",
       "  '11/15/2014',\n",
       "  '5/11/2014',\n",
       "  '3/27/2015',\n",
       "  '5/21/2013',\n",
       "  '7/30/2013',\n",
       "  '6/17/2014'],\n",
       " 'Dept': ['IT',\n",
       "  'Operations',\n",
       "  'IT',\n",
       "  'HR',\n",
       "  'Finance',\n",
       "  'IT',\n",
       "  'Operations',\n",
       "  'Finance']}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dept': ['IT',\n",
      "          'Operations',\n",
      "          'IT',\n",
      "          'HR',\n",
      "          'Finance',\n",
      "          'IT',\n",
      "          'Operations',\n",
      "          'Finance'],\n",
      " 'Name': ['Rick', 'Dan', 'Michelle', 'Ryan', 'Gary', 'Nina', 'Simon', 'Guru'],\n",
      " 'Salary': ['623.3', '515.2', '611', '729', '843.25', '578', '632.8', '722.5'],\n",
      " 'StartDate': ['1/1/2012',\n",
      "               '9/23/2013',\n",
      "               '11/15/2014',\n",
      "               '5/11/2014',\n",
      "               '3/27/2015',\n",
      "               '5/21/2013',\n",
      "               '7/30/2013',\n",
      "               '6/17/2014']}\n"
     ]
    }
   ],
   "source": [
    "x = pprint.pformat(emp, indent=1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "text4 = 'Tutorialspoint - simple easy learning.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tutorialspoint - Simple Easy Learning.'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.capwords(text4, sep=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TUTORIALSPOINT - SIMPLE EASY LEARNING.'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text4.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{116: 119, 112: 120, 111: 121, 108: 122}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transtable = text4.maketrans('tpol', 'wxyz')\n",
    "transtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tuwyriazsxyinw - simxze easy zearning.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text4.translate(transtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spell Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proud\n",
      "{'proud', 'groun', 'grout', 'group', 'aroud', 'grodd', 'ground'}\n",
      "walk\n",
      "{'wlat', 'wak', 'alak', 'flak', 'weak', 'walk', 'blak'}\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "# find those words that may be misspelled\n",
    "misspelled = spell.unknown(['Let', 'us', 'wlak','on','the','groud'])\n",
    "\n",
    "for word in misspelled:\n",
    "    # Get the one `most likely` answer\n",
    "    print(spell.correction(word))\n",
    "\n",
    "    # Get a list of `likely` options\n",
    "    print(spell.candidates(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDFs\n",
    "\n",
    "Source(s):\n",
    "- https://automatetheboringstuff.com/chapter13/\n",
    "- https://www.geeksforgeeks.org/working-with-pdf-files-in-python/\n",
    "- https://realpython.com/pdf-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "# pdfName = 'data\\CA Data Science Resume.pdf'\n",
    "pdfFileObj = open('data\\CA Data Science Resume.pdf', 'rb')\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfReader.numPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfReader.isEncrypted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdfReader.decrypt('pwd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One PDF page\n",
    "page = pdfReader.getPage(0)\n",
    "page_content = page.extractText()\n",
    "# page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page No - 1\n",
      "Page No - 2\n"
     ]
    }
   ],
   "source": [
    "# Multiple PDF pages\n",
    "for i in range(pdfReader.getNumPages()):\n",
    "    page = pdfReader.getPage(i)\n",
    "    print('Page No - ' + str(1 + pdfReader.getPageNumber(page)))\n",
    "    page_content = page.extractText()\n",
    "    # print(page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Document\n",
    "\n",
    "Source(s):\n",
    "- https://automatetheboringstuff.com/chapter13/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx  # python-docx\n",
    "\n",
    "file = 'data\\CA Data Science Resume.docx'\n",
    "\n",
    "doc = docx.Document(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc.paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Corey Atkins'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.paragraphs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc.paragraphs[1].runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Qualifications Summary'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.paragraphs[3].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in doc.paragraphs:\n",
    "#     print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getText(filename):\n",
    "    doc = docx.Document(filename)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return '\\n'.join(fullText)\n",
    "\n",
    "# print(getText(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RSS Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import feedparser\n",
    "\n",
    "marvel = 'http://www.marvel.com/feeds/rss/movies_news'\n",
    "test = 'https://timesofindia.indiatimes.com/rssfeedstopstories.cms'\n",
    "\n",
    "NewsFeed = feedparser.parse(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feed': {'language': 'en-gb',\n",
       "  'links': [{'type': 'application/rss+xml',\n",
       "    'rel': 'self',\n",
       "    'href': 'https://timesofindia.indiatimes.com/rssfeedstopstories.cms'},\n",
       "   {'rel': 'alternate',\n",
       "    'type': 'text/html',\n",
       "    'href': 'https://timesofindia.indiatimes.com'}],\n",
       "  'title': 'Times of India',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': 'en-US',\n",
       "   'base': 'https://timesofindia.indiatimes.com/rssfeedstopstories.cms',\n",
       "   'value': 'Times of India'},\n",
       "  'link': 'https://timesofindia.indiatimes.com',\n",
       "  'subtitle': 'The Times of India: Breaking news, views, reviews, cricket from across India',\n",
       "  'subtitle_detail': {'type': 'text/html',\n",
       "   'language': 'en-US',\n",
       "   'base': 'https://timesofindia.indiatimes.com/rssfeedstopstories.cms',\n",
       "   'value': 'The Times of India: Breaking news, views, reviews, cricket from across India'},\n",
       "  'rights': 'Copyright:(C) 2021 Bennett Coleman & Co. Ltd, http://info.indiatimes.com/terms/tou.html',\n",
       "  'rights_detail': {'type': 'text/plain',\n",
       "   'language': 'en-US',\n",
       "   'base': 'https://timesofindia.indiatimes.com/rssfeedstopstories.cms',\n",
       "   'value': 'Copyright:(C) 2021 Bennett Coleman & Co. Ltd, http://info.indiatimes.com/terms/tou.html'},\n",
       "  'docs': 'http://timescontent.com/',\n",
       "  'image': {'title': 'Times of India',\n",
       "   'title_detail': {'type': 'text/plain',\n",
       "    'language': 'en-US',\n",
       "    'base': 'https://timesofindia.indiatimes.com/rssfeedstopstories.cms',\n",
       "    'value': 'Times of India'},\n",
       "   'links': [{'rel': 'alternate',\n",
       "     'type': 'text/html',\n",
       "     'href': 'https://timesofindia.indiatimes.com'}],\n",
       "   'link': 'https://timesofindia.indiatimes.com',\n",
       "   'href': 'https://timesofindia.indiatimes.com/photo.cms?msid=507610'}},\n",
       " 'entries': [],\n",
       " 'bozo': 0,\n",
       " 'headers': {'Last-Modified': 'Thu, 11 Feb 2021 00:05:10 GMT',\n",
       "  'Server': 'nginx',\n",
       "  'Content-Type': 'text/xml;charset=UTF-8',\n",
       "  'content-msg': 'DATA_SERVED_FROM_CACHE',\n",
       "  'Content-Language': 'en-US',\n",
       "  'Vary': 'Accept-Encoding',\n",
       "  'Content-Encoding': 'gzip',\n",
       "  'Content-Length': '399',\n",
       "  'Cache-Control': 'public, must-revalidate, max-age=96',\n",
       "  'Expires': 'Thu, 11 Feb 2021 00:19:53 GMT',\n",
       "  'Date': 'Thu, 11 Feb 2021 00:18:17 GMT',\n",
       "  'Connection': 'close',\n",
       "  'Access-Control-Max-Age': '86400',\n",
       "  'Access-Control-Allow-Credentials': 'false',\n",
       "  'Access-Control-Allow-Headers': 'Origin,X-Requested-With,Content-Type,Accept',\n",
       "  'Access-Control-Allow-Methods': 'GET,POST',\n",
       "  'Strict-Transport-Security': 'max-age=86400'},\n",
       " 'updated': 'Thu, 11 Feb 2021 00:05:10 GMT',\n",
       " 'updated_parsed': time.struct_time(tm_year=2021, tm_mon=2, tm_mday=11, tm_hour=0, tm_min=5, tm_sec=10, tm_wday=3, tm_yday=42, tm_isdst=0),\n",
       " 'href': 'https://timesofindia.indiatimes.com/rssfeedstopstories.cms',\n",
       " 'status': 200,\n",
       " 'encoding': 'UTF-8',\n",
       " 'version': 'rss20',\n",
       " 'namespaces': {'': 'http://www.w3.org/2005/Atom'}}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NewsFeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
